{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbc62114",
   "metadata": {},
   "source": [
    "# Lab 5: Model Optimization and Performance Evaluation for Embedded AI\n",
    "\n",
    "This lab extends Lab 4 by exporting YOLOv8x to ONNX, optimizing with TensorRT and torch2trt, running live object detection, benchmarking Precision/Recall/F1 and mAP, and profiling with NVIDIA tools [<a href='#ref-1'>1</a>, <a href='#ref-2'>2</a>, <a href='#ref-3'>3</a>, <a href='#ref-4'>4</a>, <a href='#ref-5'>5</a>, <a href='#ref-6'>6</a>, <a href='#ref-7'>7</a>, <a href='#ref-8'>8</a>, <a href='#ref-10'>10</a>].\n",
    "\n",
    "By the end of this lab, students will be able to: [<a href='#ref-1'>1</a>]\n",
    "\n",
    "- Export YOLOv8x from PyTorch to ONNX and build a TensorRT engine [<a href='#ref-1'>1</a>, <a href='#ref-2'>2</a>, <a href='#ref-4'>4</a>]\n",
    "- Optimize a YOLOv8x PyTorch module using torch2trt and save the engine/module state [<a href='#ref-6'>6</a>]\n",
    "- Run object detection and record throughput (FPS), latency, and task performance metrics (Precision, Recall, F1, mAP@0.50, mAP@0.50–0.95) for PyTorch, TensorRT (engine), and torch2trt [<a href='#ref-1'>1</a>]\n",
    "- Compare and anylyze interpret results [<a href='#ref-9'>9</a>]\n",
    "- Profile with Nsight Systems/Compute and the TensorRT profiler; visualize findings and summarize bottlenecks [<a href='#ref-7'>7</a>, <a href='#ref-8'>8</a>, <a href='#ref-5'>5</a>]\n",
    "\n",
    "Assumptions: reuse the Lab 4 Docker image, where all tools are already installed [<a href='#ref-1'>1</a>]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447f0770",
   "metadata": {},
   "source": [
    "## Folder Structure and Configuration [<a href='#ref-1'>1</a>]\n",
    "\n",
    "- Create a working folder named `Lab5` inside your Jetson lab directory `~/CMPE6012/STUDENT_ID`  [<a href='#ref-1'>1</a>].\n",
    "```bash\n",
    "mkdir -p ~/CMPE6012/STUDENT_ID/Lab5 # Replace STUDENT_ID with your student ID\n",
    "```\n",
    "- Create the following files into `Lab5` and run them as indicated in later sections [<a href='#ref-1'>1</a>]:\n",
    "  - config.py\n",
    "  - baseline_infer_pytorch.py\n",
    "  - task1_export_onnx.py\n",
    "  - task1_build_trt_engine.py\n",
    "  - task1_infer_trt.py\n",
    "  - task2_optimize_torch2trt.py\n",
    "  - task2_infer_torch2trt.py\n",
    "  - task3_optimize_torch2trt.py\n",
    "  - task3_infer_torch2trt.py\n",
    "  - visualize_efficiency.py\n",
    "  - visualize_task_performance.py\n",
    "  - profile_wrappers.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86890d3a",
   "metadata": {},
   "source": [
    "### Create a config file for global settings across all tasks in Lab 5\n",
    "config.py [<a href='#ref-1'>1</a>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede5dd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAB_DIR/config.py\n",
    "from pathlib import Path\n",
    "\n",
    "LAB_DIR = Path.cwd()  # or Path('/workspace') inside container\n",
    "LAB_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Model and I/O\n",
    "MODEL_PT = '../Lab4/yolov8x.pt'\n",
    "IMG_SIZE = 640\n",
    "BATCH = 1\n",
    "DYNAMIC = False\n",
    "HALF = False\n",
    "\n",
    "# Artifacts\n",
    "ONNX_PATH = LAB_DIR / 'yolov8x.onnx'\n",
    "ENGINE_PATH = LAB_DIR / 'yolov8x_trt_from_onnx.engine'\n",
    "ONNX_PATH_FP16 = LAB_DIR / 'yolov8x_fp16.onnx'\n",
    "ENGINE_PATH_FP16 = LAB_DIR / 'yolov8x_trt_from_onnx_fp16.engine'\n",
    "\n",
    "ENGINE_PATH_T2T = LAB_DIR / 'yolov8x_trt_from_torch2trt.engine'\n",
    "ENGINE_PATH_T2T_FP16 = LAB_DIR / 'yolov8x_trt_from_torch2trt_fp16.engine'\n",
    "T2TRT_STATE = LAB_DIR / 'yolov8x_torch2trt.pth'\n",
    "T2TRT_STATE_FP16 = LAB_DIR / 'yolov8x_torch2trt_fp16.pth'\n",
    "\n",
    "# Dataset\n",
    "DATA_YAML = '/datasets/coco/coco_mini_val.yaml'\n",
    "\n",
    "# Camera\n",
    "CAMERA_INDEX = 0  # replace with GStreamer string on Jetson if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9302bc",
   "metadata": {},
   "source": [
    "## Task 0: Baseline PyTorch Live Inference [<a href='#ref-1'>1</a>, <a href='#ref-3'>3</a>]\n",
    "\n",
    "Instructions: [<a href='#ref-1'>1</a>]\n",
    "\n",
    "- Create and run the baseline inference script to establish reference FPS and latency [<a href='#ref-1'>1</a>].\n",
    "- Keep the same dataset and image size for fair comparison across tasks [<a href='#ref-1'>1</a>].\n",
    "- You can also revise your Task 3.4 implementation in Lab 4 to your baseline for Lab 5.\n",
    "- Record the reference metrics, you will need these for later comparisons.\n",
    "- Questions to think about:\n",
    "1. What is `p95 latency`?\n",
    "2. What are the differences between `mean latency` and `p95 latency`?\n",
    "3. Why percentile latency matters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8a1265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAB_DIR/baseline_infer_pytorch.py\n",
    "import time, numpy as np, cv2, argparse, json\n",
    "from ultralytics import YOLO\n",
    "from config import MODEL_PT, IMG_SIZE, CAMERA_INDEX, DATA_YAML\n",
    "\n",
    "def infer_live(camera_index, metrics_out='baseline_live_metrics.json'):\n",
    "    model = YOLO(MODEL_PT)\n",
    "    cap = cv2.VideoCapture(camera_index)\n",
    "    if not cap.isOpened(): raise RuntimeError('Camera not available')\n",
    "    latencies = []; t_last = time.perf_counter(); fps_smooth, alpha = 0.0, 0.1\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok: break\n",
    "        t0 = time.perf_counter()\n",
    "        results = model.predict(source=frame, imgsz=IMG_SIZE, verbose=False, device=0)\n",
    "        t1 = time.perf_counter()\n",
    "        lat_ms = (t1 - t0) * 1000.0; latencies.append(lat_ms)\n",
    "        annotated = results[0].plot()\n",
    "        dt = t1 - t_last; inst_fps = 1.0 / max(dt, 1e-6)\n",
    "        fps_smooth = (1 - alpha)*fps_smooth + alpha*inst_fps if fps_smooth > 0 else inst_fps\n",
    "        t_last = t1\n",
    "        cv2.putText(annotated, f'Latency: {lat_ms:.1f} ms  FPS: {fps_smooth:.1f}', (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,255,0), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('PyTorch YOLOv8x (Baseline)', annotated)\n",
    "        if cv2.waitKey(1) & 0xFF == 27: break\n",
    "    cap.release(); cv2.destroyAllWindows()\n",
    "    if latencies:\n",
    "        metrics = {\n",
    "            'mean_latency_ms': float(np.mean(latencies)),\n",
    "            'p95_latency_ms': float(np.percentile(latencies, 95)),\n",
    "            'frames': len(latencies),\n",
    "            'fps': len(latencies) / (np.sum(latencies)/1000.0)\n",
    "        }\n",
    "        print(metrics)\n",
    "        with open(metrics_out, 'w') as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "        print(f\"Saved live metrics to {metrics_out}\")\n",
    "\n",
    "def infer_coco(metrics_out='baseline_coco_metrics.json'):\n",
    "    model = YOLO(MODEL_PT)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    results = model.val(data=DATA_YAML, imgsz=IMG_SIZE, device=0, verbose=False)\n",
    "    t1 = time.perf_counter()\n",
    "    latency_ms = (t1 - t0) * 1000.0\n",
    "\n",
    "    def to_scalar(x):\n",
    "        a = np.asarray(x)\n",
    "        if a.size == 0:\n",
    "            return float('nan')\n",
    "        if a.shape == ():\n",
    "            return float(a)\n",
    "        return float(np.nanmean(a))\n",
    "\n",
    "    # Pull metrics with array-safe conversion\n",
    "    try:\n",
    "        precision = to_scalar(results.box.p)\n",
    "        recall = to_scalar(results.box.r)\n",
    "        # f1 may be missing sometimes, compute if needed\n",
    "        f1 = to_scalar(getattr(results.box, 'f1', np.array([2 * precision * recall / max(precision + recall, 1e-12)])))\n",
    "        map50 = to_scalar(results.box.map50)\n",
    "        map50_95 = to_scalar(results.box.map)\n",
    "    except AttributeError:\n",
    "        # Fallback for older APIs\n",
    "        p, r, map50_val, map50_95_val = results.mean_results()\n",
    "        precision = float(p); recall = float(r)\n",
    "        map50 = float(map50_val); map50_95 = float(map50_95_val)\n",
    "        f1 = float(2 * precision * recall / max(precision + recall, 1e-12))\n",
    "\n",
    "    # FPS from reported per-image inference speed when available\n",
    "    try:\n",
    "        inf_ms = float(results.speed.get('inference', 0.0))\n",
    "        fps = 1000.0 / inf_ms if inf_ms > 0 else 1.0 / max((t1 - t0), 1e-6)\n",
    "    except Exception:\n",
    "        fps = 1.0 / max((t1 - t0), 1e-6)\n",
    "\n",
    "    metrics = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'map50': map50,\n",
    "        'map50_95': map50_95,\n",
    "        'mean_latency_ms': float(latency_ms),\n",
    "        'p95_latency_ms': float(latency_ms),  # single pass\n",
    "        'fps': float(fps),\n",
    "    }\n",
    "\n",
    "    print(metrics)\n",
    "    with open(metrics_out, 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(f\"Saved COCO metrics to {metrics_out}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--mode', choices=['live', 'coco'], default='coco')\n",
    "    parser.add_argument('--camera', type=int, default=CAMERA_INDEX)\n",
    "    parser.add_argument('--metrics_out', type=str, default=None)\n",
    "    args = parser.parse_args()\n",
    "    if args.mode == 'live':\n",
    "        out = args.metrics_out or 'baseline_live_metrics.json'\n",
    "        infer_live(args.camera, metrics_out=out)\n",
    "    else:\n",
    "        out = args.metrics_out or 'baseline_coco_metrics.json'\n",
    "        infer_coco(metrics_out=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0275b3c4",
   "metadata": {},
   "source": [
    "## Task 1: PyTorch → ONNX → TensorRT [<a href='#ref-1'>1</a>, <a href='#ref-2'>2</a>, <a href='#ref-4'>4</a>]\n",
    "\n",
    "Instructions: [<a href='#ref-1'>1</a>]\n",
    "\n",
    "- Create and run the export and engine-build scripts in `Lab5`; verify .onnx and .engine files are produced [<a href='#ref-1'>1</a>, <a href='#ref-2'>2</a>].\n",
    "- Create and run the inference script with the .engine and observe the metrics [<a href='#ref-1'>1</a>].\n",
    "- Record results for comparison with Task 2 and the baseline (Task 0) [<a href='#ref-1'>1</a>]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5161c729",
   "metadata": {},
   "source": [
    "### task1_export_onnx.py [<a href='#ref-1'>1</a>, <a href='#ref-2'>2</a>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82df21db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAB_DIR/task1_export_onnx.py\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "from config import MODEL_PT, IMG_SIZE, DYNAMIC, HALF, ONNX_PATH\n",
    "\n",
    "def main():\n",
    "    model = YOLO(MODEL_PT)\n",
    "    exported = model.export(format='onnx', imgsz=IMG_SIZE, dynamic=DYNAMIC, half=HALF)\n",
    "    src = Path(str(exported))\n",
    "    if src.exists() and src.resolve() != ONNX_PATH.resolve():\n",
    "        src.rename(ONNX_PATH)\n",
    "    print(f'ONNX saved at: {ONNX_PATH}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a659994",
   "metadata": {},
   "source": [
    "### task1_build_trt_engine.py [<a href='#ref-2'>2</a>, <a href='#ref-4'>4</a>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fe56b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAB_DIR/task1_build_trt_engine.py\n",
    "from ultralytics.utils.export import export_engine\n",
    "from config import ONNX_PATH, ENGINE_PATH, BATCH, IMG_SIZE, DYNAMIC\n",
    "\n",
    "def main():\n",
    "    assert ONNX_PATH.exists(), 'ONNX file not found'\n",
    "    export_engine(\n",
    "        onnx_file=str(ONNX_PATH),\n",
    "        engine_file=str(ENGINE_PATH),\n",
    "        workspace=4,\n",
    "        half=False,\n",
    "        int8=False,\n",
    "        dynamic=DYNAMIC,\n",
    "        shape=(BATCH, 3, IMG_SIZE, IMG_SIZE),\n",
    "        verbose=True\n",
    "    )\n",
    "    print(f'Engine saved at: {ENGINE_PATH}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba75f73",
   "metadata": {},
   "source": [
    "### task1_infer_trt.py [<a href='#ref-1'>1</a>, <a href='#ref-6'>6</a>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6004f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAB_DIR/task1_infer_trt.py\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import argparse\n",
    "import json\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "from config import ENGINE_PATH, IMG_SIZE, CAMERA_INDEX, DATA_YAML\n",
    "\n",
    "# Fix deprecated np.bool alias\n",
    "if not hasattr(np, \"bool\"):\n",
    "    np.bool = np.bool_\n",
    "\n",
    "def xyxy_iou(box1, box2):\n",
    "    \"\"\"Compute IoU between two sets of boxes in xyxy format.\"\"\"\n",
    "    if len(box1) == 0 or len(box2) == 0:\n",
    "        return np.zeros((len(box1), len(box2)), dtype=np.float32)\n",
    "\n",
    "    x1 = np.maximum(box1[:, None, 0], box2[None, :, 0])\n",
    "    y1 = np.maximum(box1[:, None, 1], box2[None, :, 1])\n",
    "    x2 = np.minimum(box1[:, None, 2], box2[None, :, 2])\n",
    "    y2 = np.minimum(box1[:, None, 3], box2[None, :, 3])\n",
    "\n",
    "    inter_area = np.clip(x2 - x1, 0, None) * np.clip(y2 - y1, 0, None)\n",
    "    box1_area = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])\n",
    "    box2_area = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])\n",
    "    union_area = box1_area[:, None] + box2_area[None, :] - inter_area\n",
    "    iou = inter_area / np.clip(union_area, 1e-6, None)\n",
    "    return iou\n",
    "\n",
    "def load_yolo_dataset(yaml_file):\n",
    "    \"\"\"Load YOLO dataset from YAML file, supporting relative label folder.\"\"\"\n",
    "    with open(yaml_file, 'r') as f:\n",
    "        data_cfg = yaml.safe_load(f)\n",
    "\n",
    "    dataset_root = Path(data_cfg['path']).resolve()\n",
    "    val_file = Path(data_cfg['val']).resolve()\n",
    "    if not val_file.exists():\n",
    "        val_file = dataset_root / data_cfg['val']\n",
    "    if not val_file.exists():\n",
    "        raise FileNotFoundError(f\"Validation file not found: {val_file}\")\n",
    "\n",
    "    dataset = []\n",
    "    with open(val_file, 'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "\n",
    "    for line in lines:\n",
    "        img_path = (dataset_root / line).resolve()\n",
    "        if not img_path.exists():\n",
    "            print(f\"Warning: image not found: {img_path}\")\n",
    "            continue\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None:\n",
    "            continue\n",
    "        h, w = img.shape[:2]\n",
    "\n",
    "        # Correct relative label path\n",
    "        label_path = (img_path.parent.parent.parent / \"labels/val2017\" / img_path.name).with_suffix('.txt')\n",
    "        boxes = []\n",
    "        if label_path.exists():\n",
    "            with open(label_path, 'r') as lf:\n",
    "                for lbl in lf:\n",
    "                    cls, x_center, y_center, bw, bh = map(float, lbl.strip().split())\n",
    "                    x1 = (x_center - bw/2) * w\n",
    "                    y1 = (y_center - bh/2) * h\n",
    "                    x2 = (x_center + bw/2) * w\n",
    "                    y2 = (y_center + bh/2) * h\n",
    "                    boxes.append([x1, y1, x2, y2])\n",
    "        else:\n",
    "            print(f\"Warning: label file not found: {label_path}\")\n",
    "\n",
    "        dataset.append((str(img_path), np.array(boxes)))\n",
    "\n",
    "    print(f\"Found {len(dataset)} validation images.\")\n",
    "    return dataset\n",
    "\n",
    "def infer_live(camera_index: int, metrics_out: str = 'trt_live_metrics.json'):\n",
    "    assert ENGINE_PATH.exists(), f'TensorRT engine not found: {ENGINE_PATH}'\n",
    "    model = YOLO(str(ENGINE_PATH))\n",
    "\n",
    "    cap = cv2.VideoCapture(camera_index)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f'Camera {camera_index} not available')\n",
    "\n",
    "    latencies = []\n",
    "    t_last, fps_smooth, alpha = time.perf_counter(), 0.0, 0.1\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        results = model.predict(source=frame, imgsz=IMG_SIZE, verbose=False, device=0)\n",
    "        t1 = time.perf_counter()\n",
    "\n",
    "        lat_ms = (t1 - t0) * 1000.0\n",
    "        latencies.append(lat_ms)\n",
    "\n",
    "        annotated = results[0].plot()\n",
    "        inst_fps = 1.0 / max(t1 - t_last, 1e-6)\n",
    "        fps_smooth = inst_fps if fps_smooth == 0 else (1 - alpha) * fps_smooth + alpha * inst_fps\n",
    "        t_last = t1\n",
    "\n",
    "        cv2.putText(\n",
    "            annotated,\n",
    "            f'Latency: {lat_ms:.1f} ms  FPS: {fps_smooth:.1f}',\n",
    "            (10, 30),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.9,\n",
    "            (0, 255, 0),\n",
    "            2,\n",
    "            cv2.LINE_AA\n",
    "        )\n",
    "        cv2.imshow('TensorRT YOLOv8x', annotated)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    if latencies:\n",
    "        metrics = {\n",
    "            'mean_latency_ms': float(np.mean(latencies)),\n",
    "            'p95_latency_ms': float(np.percentile(latencies, 95)),\n",
    "            'frames': len(latencies),\n",
    "            'fps': len(latencies) / (np.sum(latencies) / 1000.0)\n",
    "        }\n",
    "    else:\n",
    "        metrics = {\n",
    "            'mean_latency_ms': float('nan'),\n",
    "            'p95_latency_ms': float('nan'),\n",
    "            'frames': 0,\n",
    "            'fps': float('nan')\n",
    "        }\n",
    "\n",
    "    print(metrics)\n",
    "    with open(metrics_out, 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(f\"Saved live metrics to {metrics_out}\")\n",
    "\n",
    "def infer_coco(metrics_out: str = 'trt_coco_metrics.json'):\n",
    "    assert ENGINE_PATH.exists(), f'TensorRT engine not found: {ENGINE_PATH}'\n",
    "    model = YOLO(str(ENGINE_PATH))\n",
    "\n",
    "    dataset = load_yolo_dataset(DATA_YAML)\n",
    "\n",
    "    if len(dataset) == 0:\n",
    "        print(\"Warning: No validation images found!\")\n",
    "        metrics = {\n",
    "            'precision': float('nan'),\n",
    "            'recall': float('nan'),\n",
    "            'f1': float('nan'),\n",
    "            'mean_latency_ms': float('nan'),\n",
    "            'p95_latency_ms': float('nan'),\n",
    "            'frames': 0,\n",
    "            'fps': float('nan')\n",
    "        }\n",
    "        with open(metrics_out, 'w') as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "        return\n",
    "\n",
    "    latencies = []\n",
    "    all_precisions, all_recalls = [], []\n",
    "\n",
    "    for img_path, gt_boxes in dataset:\n",
    "        t0 = time.perf_counter()\n",
    "        results = model.predict(source=img_path, imgsz=IMG_SIZE, verbose=False, device=0)\n",
    "        t1 = time.perf_counter()\n",
    "        latencies.append((t1 - t0) * 1000.0)\n",
    "\n",
    "        pred_boxes = results[0].boxes.xyxy.cpu().numpy() if results[0].boxes else np.zeros((0, 4))\n",
    "\n",
    "        if len(pred_boxes) > 0 and len(gt_boxes) > 0:\n",
    "            ious = xyxy_iou(pred_boxes, gt_boxes)\n",
    "            tp = (ious >= 0.5).sum()\n",
    "            fp = len(pred_boxes) - tp\n",
    "            fn = len(gt_boxes) - tp\n",
    "            precision = tp / max(tp + fp, 1e-6)\n",
    "            recall = tp / max(tp + fn, 1e-6)\n",
    "        else:\n",
    "            precision, recall = 0.0, 0.0\n",
    "\n",
    "        all_precisions.append(precision)\n",
    "        all_recalls.append(recall)\n",
    "\n",
    "    mean_precision = float(np.mean(all_precisions))\n",
    "    mean_recall = float(np.mean(all_recalls))\n",
    "    f1 = 2 * mean_precision * mean_recall / max(mean_precision + mean_recall, 1e-12)\n",
    "    mean_latency = float(np.mean(latencies))\n",
    "    fps = len(latencies) / (np.sum(latencies) / 1000.0)\n",
    "\n",
    "    metrics = {\n",
    "        'precision': mean_precision,\n",
    "        'recall': mean_recall,\n",
    "        'f1': f1,\n",
    "        'mean_latency_ms': mean_latency,\n",
    "        'p95_latency_ms': float(np.percentile(latencies, 95)),\n",
    "        'frames': len(latencies),\n",
    "        'fps': fps\n",
    "    }\n",
    "\n",
    "    print(metrics)\n",
    "    with open(metrics_out, 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(f\"Saved dataset metrics to {metrics_out}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description=\"TensorRT YOLOv8 inference\")\n",
    "    parser.add_argument('--mode', choices=['live', 'coco'], default='coco')\n",
    "    parser.add_argument('--camera', type=int, default=CAMERA_INDEX)\n",
    "    parser.add_argument('--metrics_out', type=str, default=None)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    out_file = args.metrics_out or ('trt_live_metrics.json' if args.mode == 'live' else 'trt_coco_metrics.json')\n",
    "    if args.mode == 'live':\n",
    "        infer_live(args.camera, metrics_out=out_file)\n",
    "    else:\n",
    "        infer_coco(metrics_out=out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa13eb21",
   "metadata": {},
   "source": [
    "## (Ignore) Task 2: torch2trt Optimization [<a href='#ref-6'>6</a>]\n",
    "\n",
    "*Ignore this task as torch2trt is not compatible to YOLO.*\n",
    "\n",
    "Instructions: [<a href='#ref-6'>6</a>]\n",
    "\n",
    "- Create and run the optimization script to save the TensorRT engine and TRTModule state for YOLOv8x [<a href='#ref-6'>6</a>].\n",
    "- Create and run the inference script for the torch2trt module and collect all metrics [<a href='#ref-6'>6</a>].\n",
    "- Record and compare results against Task 1 and the baseline (Task 0) [<a href='#ref-1'>1</a>]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a20e7d",
   "metadata": {},
   "source": [
    "### task2_optimize_torch2trt.py [<a href='#ref-6'>6</a>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e8c2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5e855d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAB_DIR/task2_optimize_torch2trt.py\n",
    "import torch\n",
    "from torch2trt import torch2trt, TRTModule\n",
    "from ultralytics import YOLO\n",
    "from config import MODEL_PT, IMG_SIZE, BATCH, T2TRT_STATE\n",
    "\n",
    "def main():\n",
    "    y = YOLO(MODEL_PT)\n",
    "    core = y.model.eval().cuda()\n",
    "    example = torch.randn(BATCH, 3, IMG_SIZE, IMG_SIZE).cuda()\n",
    "    model_trt = torch2trt(core, [example], fp16_mode=False, max_batch_size=BATCH)\n",
    "    torch.save(model_trt.state_dict(), str(T2TRT_STATE))\n",
    "    print(f'torch2trt state saved at: {T2TRT_STATE}')\n",
    "    # Save raw TensorRT engine file for trtexec\n",
    "    with open(ENGINE_PATH_T2T, \"wb\") as f:\n",
    "        f.write(model_trt.engine.serialize())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7c87c1",
   "metadata": {},
   "source": [
    "### task2_infer_torch2trt.py [<a href='#ref-6'>6</a>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cde346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAB_DIR/task2_infer_torch2trt.py\n",
    "import time, numpy as np, cv2, argparse, torch, json\n",
    "from torchvision.ops import nms\n",
    "from torch2trt import TRTModule\n",
    "from config import T2TRT_STATE, IMG_SIZE, CAMERA_INDEX, DATA_YAML\n",
    "\n",
    "def preprocess_bgr(frame, size=IMG_SIZE):\n",
    "    img = cv2.resize(frame, (size, size), interpolation=cv2.INTER_LINEAR)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    t = torch.from_numpy(img).permute(2,0,1).float().cuda() / 255.0\n",
    "    return t.unsqueeze(0), img\n",
    "\n",
    "def postprocess_yolov8(pred, conf_thres=0.25, iou_thres=0.45):\n",
    "    if isinstance(pred, (list, tuple)): pred = pred[0]\n",
    "    if pred.ndim == 3: pred = pred[0]\n",
    "    p = pred.detach().float()\n",
    "    if p.shape[-1] < 6: return []\n",
    "    boxes_cxcywh = p[:, :4]\n",
    "    objectness = p[:, 4:5]\n",
    "    class_scores = p[:, 5:]\n",
    "    cls_conf, cls_idx = class_scores.max(dim=1, keepdim=True)\n",
    "    conf = (objectness * cls_conf).squeeze(1)\n",
    "    keep = conf > conf_thres\n",
    "    if keep.sum() == 0: return []\n",
    "    boxes_cxcywh, conf, cls_idx = boxes_cxcywh[keep], conf[keep], cls_idx[keep].squeeze(1)\n",
    "    cx, cy, w, h = boxes_cxcywh.t()\n",
    "    x1, y1, x2, y2 = cx - w/2, cy - h/2, cx + w/2, cy + h/2\n",
    "    boxes_xyxy = torch.stack([x1, y1, x2, y2], dim=1)\n",
    "    keep_idx = nms(boxes_xyxy, conf, iou_thres)\n",
    "    return boxes_xyxy[keep_idx], conf[keep_idx], cls_idx[keep_idx]\n",
    "\n",
    "def infer_live(camera_index, metrics_out='torch2trt_live_metrics.json'):\n",
    "    tloaded = TRTModule()\n",
    "    tloaded.load_state_dict(torch.load(str(T2TRT_STATE)))\n",
    "    tloaded.eval().cuda()\n",
    "    cap = cv2.VideoCapture(camera_index)\n",
    "    if not cap.isOpened(): raise RuntimeError('Camera not available')\n",
    "    latencies = []; t_last = time.perf_counter(); fps_smooth, alpha = 0.0, 0.1\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok: break\n",
    "        h0, w0 = frame.shape[:2]\n",
    "        t_in, _ = preprocess_bgr(frame, IMG_SIZE)\n",
    "        t0 = time.perf_counter()\n",
    "        with torch.inference_mode():\n",
    "            y = tloaded(t_in)\n",
    "        t1 = time.perf_counter()\n",
    "        lat_ms = (t1 - t0) * 1000.0; latencies.append(lat_ms)\n",
    "        parsed = postprocess_yolov8(y)\n",
    "        annotated = frame.copy()\n",
    "        if parsed:\n",
    "            boxes, confs, clss = parsed\n",
    "            sx, sy = w0 / IMG_SIZE, h0 / IMG_SIZE\n",
    "            for b, c, k in zip(boxes, confs, clss):\n",
    "                x1, y1, x2, y2 = b.tolist()\n",
    "                x1, x2 = int(x1*sx), int(x2*sx); y1, y2 = int(y1*sy), int(y2*sy)\n",
    "                cv2.rectangle(annotated, (x1,y1), (x2,y2), (0,255,0), 2)\n",
    "                cv2.putText(annotated, f'{int(k)}:{c:.2f}', (x1, max(0,y1-5)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2, cv2.LINE_AA)\n",
    "        dt = t1 - t_last; inst_fps = 1.0 / max(dt, 1e-6)\n",
    "        fps_smooth = (1 - alpha)*fps_smooth + alpha*inst_fps if fps_smooth > 0 else inst_fps\n",
    "        t_last = t1\n",
    "        cv2.putText(annotated, f'Latency: {lat_ms:.1f} ms  FPS: {fps_smooth:.1f}', (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,255,0), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('torch2trt (Task 2)', annotated)\n",
    "        if cv2.waitKey(1) & 0xFF == 27: break\n",
    "    cap.release(); cv2.destroyAllWindows()\n",
    "    if latencies:\n",
    "        metrics = {\n",
    "            'mean_latency_ms': float(np.mean(latencies)),\n",
    "            'p95_latency_ms': float(np.percentile(latencies, 95)),\n",
    "            'frames': len(latencies),\n",
    "            'fps': len(latencies) / (np.sum(latencies)/1000.0)\n",
    "        }\n",
    "        print(metrics)\n",
    "        with open(metrics_out, 'w') as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "        print(f\"Saved live metrics to {metrics_out}\")\n",
    "\n",
    "def infer_coco(metrics_out='torch2trt_coco_metrics.json'):\n",
    "    from pathlib import Path\n",
    "    from ultralytics.utils.ops import box_iou\n",
    "    import os\n",
    "\n",
    "    MINI_LIST = \"/datasets/coco/coco_mini_val.txt\"\n",
    "    if Path(MINI_LIST).exists():\n",
    "        with open(MINI_LIST) as f:\n",
    "            image_paths = [l.strip() for l in f if l.strip()]\n",
    "        image_paths = [p for p in image_paths if Path(p).exists()]\n",
    "    else:\n",
    "        import yaml\n",
    "        with open(DATA_YAML) as f:\n",
    "            data = yaml.safe_load(f)\n",
    "        image_dir = Path(data['val'])\n",
    "        image_paths = sorted(list(image_dir.glob('*.jpg')))[:100]\n",
    "\n",
    "    tloaded = TRTModule()\n",
    "    tloaded.load_state_dict(torch.load(str(T2TRT_STATE)))\n",
    "    tloaded.eval().cuda()\n",
    "\n",
    "    latencies = []\n",
    "    TP = FP = FN = 0\n",
    "\n",
    "    def label_path_for(image_path: str) -> Path:\n",
    "        p = Path(image_path)\n",
    "        parts = list(p.parts)\n",
    "        for i, s in enumerate(parts):\n",
    "            if s == \"images\":\n",
    "                parts[i] = \"labels\"\n",
    "                break\n",
    "        lp = Path(*parts).with_suffix(\".txt\")\n",
    "        return lp\n",
    "\n",
    "    def load_labels_for_image(image_path: str):\n",
    "        lab = label_path_for(image_path)\n",
    "        boxes = []\n",
    "        if lab.exists():\n",
    "            with open(lab) as f:\n",
    "                for line in f:\n",
    "                    vals = line.strip().split()\n",
    "                    if len(vals) >= 5:\n",
    "                        c, cx, cy, w, h = map(float, vals[:5])\n",
    "                        boxes.append((int(c), cx, cy, w, h))\n",
    "        return boxes\n",
    "\n",
    "    def scale_yolo_to_xyxy(lbls, w, h):\n",
    "        out = []\n",
    "        for c, cx, cy, bw, bh in lbls:\n",
    "            x1 = (cx - bw/2) * w; y1 = (cy - bh/2) * h\n",
    "            x2 = (cx + bw/2) * w; y2 = (cy + bh/2) * h\n",
    "            out.append((c, x1, y1, x2, y2))\n",
    "        return out\n",
    "\n",
    "    for img_path in image_paths:\n",
    "        im0 = cv2.imread(str(img_path))\n",
    "        if im0 is None:\n",
    "            continue\n",
    "        h0, w0 = im0.shape[:2]\n",
    "        im = cv2.resize(im0, (IMG_SIZE, IMG_SIZE))\n",
    "        rgb = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "        t = torch.from_numpy(rgb).permute(2,0,1).float().cuda().unsqueeze(0) / 255.0\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        with torch.inference_mode():\n",
    "            pred = tloaded(t)\n",
    "        t1 = time.perf_counter()\n",
    "        latencies.append((t1 - t0) * 1000.0)\n",
    "\n",
    "        if isinstance(pred, (list, tuple)): pred = pred[0]\n",
    "        if pred.ndim == 3: pred = pred[0]\n",
    "        if pred.shape[-1] < 6:\n",
    "            gt = scale_yolo_to_xyxy(load_labels_for_image(img_path), w0, h0)\n",
    "            FN += len(gt)\n",
    "            continue\n",
    "\n",
    "        boxes_cxcywh = pred[:, :4]\n",
    "        conf = (pred[:, 4:5] * pred[:, 5:].max(dim=1, keepdim=True)).squeeze(1)\n",
    "        keep = conf > 0.25\n",
    "        boxes_cxcywh = boxes_cxcywh[keep]; conf = conf[keep]\n",
    "        if boxes_cxcywh.numel() == 0:\n",
    "            gt = scale_yolo_to_xyxy(load_labels_for_image(img_path), w0, h0)\n",
    "            FN += len(gt)\n",
    "            continue\n",
    "        cx, cy, w, h = boxes_cxcywh.t()\n",
    "        x1 = (cx - w/2) * (w0 / IMG_SIZE); y1 = (cy - h/2) * (h0 / IMG_SIZE)\n",
    "        x2 = (cx + w/2) * (w0 / IMG_SIZE); y2 = (cy + h/2) * (h0 / IMG_SIZE)\n",
    "        det = torch.stack([x1, y1, x2, y2], dim=1)\n",
    "\n",
    "        gt_list = scale_yolo_to_xyxy(load_labels_for_image(img_path), w0, h0)\n",
    "        if len(gt_list) == 0:\n",
    "            FP += len(det)\n",
    "            continue\n",
    "        gt = torch.tensor([g[1:] for g in gt_list], dtype=torch.float32)\n",
    "        if det.numel() == 0:\n",
    "            FN += len(gt)\n",
    "            continue\n",
    "\n",
    "        ious = box_iou(det, gt)\n",
    "        matched_gt = set()\n",
    "        for i in range(det.shape[0]):\n",
    "            j = int(torch.argmax(ious[i]))\n",
    "            if ious[i, j] >= 0.5 and j not in matched_gt:\n",
    "                TP += 1; matched_gt.add(j)\n",
    "            else:\n",
    "                FP += 1\n",
    "        FN += (gt.shape[0] - len(matched_gt))\n",
    "\n",
    "    precision = TP / max(TP + FP, 1)\n",
    "    recall = TP / max(TP + FN, 1)\n",
    "    f1 = 2 * precision * recall / max(precision + recall, 1e-9)\n",
    "    metrics = {\n",
    "        'mean_latency_ms': float(np.mean(latencies)) if latencies else None,\n",
    "        'p95_latency_ms': float(np.percentile(latencies, 95)) if latencies else None,\n",
    "        'frames': len(latencies),\n",
    "        'fps': len(latencies) / (np.sum(latencies)/1000.0) if latencies else None,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "    print(metrics)\n",
    "    with open(metrics_out, 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(f\"Saved COCO metrics to {metrics_out}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--mode', choices=['live', 'coco'], default='coco')\n",
    "    parser.add_argument('--camera', type=int, default=CAMERA_INDEX)\n",
    "    parser.add_argument('--metrics_out', type=str, default=None)\n",
    "    args = parser.parse_args()\n",
    "    if args.mode == 'live':\n",
    "        out = args.metrics_out or 'torch2trt_live_metrics.json'\n",
    "        infer_live(args.camera, metrics_out=out)\n",
    "    else:\n",
    "        out = args.metrics_out or 'torch2trt_coco_metrics.json'\n",
    "        infer_coco(metrics_out=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4899d0d4",
   "metadata": {},
   "source": [
    "## Task 3: PyTorch → ONNX → TensorRT with FP16\n",
    "\n",
    "Instructions:\n",
    "- Revise the documents for Task 1 to enable `HALF`, and save the optimized model to `ONNX_PATH_FP16` and `ENGINE_PATH_FP16`.\n",
    "- Copy `task1_infer_trt.py` as `task3_infer_trt.py`, and revise accordingly to use the smaller model with FP16 and save results to `trt_fp16_live_metrics.json` or `trt_fp16_coco_metrics.jason`.\n",
    "- Run `task3_infer_trt_fp16.py` for the smaller TensorRT engine and collect all metrics.\n",
    "- Record and compare results against Task 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59446f20",
   "metadata": {},
   "source": [
    "## Task 4: Comparison through Visualization [<a href='#ref-9'>9</a>, <a href='#ref-1'>1</a>]\n",
    "\n",
    "- Plot latency series and FPS bar charts across baseline, TensorRT engine, and TensorRT engine FP16 [<a href='#ref-1'>1</a>].\n",
    "- Plot Precision/Recall/F1 versus confidence across baseline, TensorRT engine, and TensorRT engine FP16 [<a href='#ref-9'>9</a>].\n",
    "- Observe: how these metrics change across all tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c78e2c",
   "metadata": {},
   "source": [
    "### visualize_efficiency.py (Latency/FPS) [<a href='#ref-1'>1</a>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b3c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAB_DIR/visualize_efficiency.py\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "def plot_latency_fps_bar(metrics_files, labels, out_png):\n",
    "    means, p95s, fps = [], [], []\n",
    "    for f in metrics_files:\n",
    "        with open(f) as jf:\n",
    "            m = json.load(jf)\n",
    "            means.append(m.get('mean_latency_ms', 0))\n",
    "            p95s.append(m.get('p95_latency_ms', 0))\n",
    "            fps.append(m.get('fps', 0))\n",
    "    x = np.arange(len(labels))\n",
    "    fig, ax = plt.subplots(1,2,figsize=(12,5))\n",
    "    ax[0].bar(x, means, label='Mean Latency')\n",
    "    ax[0].bar(x, p95s, bottom=means, label='P95 Latency')\n",
    "    ax[0].set_xticks(x); ax[0].set_xticklabels(labels, rotation=15)\n",
    "    ax[0].set_ylabel('Latency (ms)'); ax[0].legend()\n",
    "    ax[1].bar(x, fps)\n",
    "    ax[1].set_xticks(x); ax[1].set_xticklabels(labels, rotation=15)\n",
    "    ax[1].set_ylabel('FPS')\n",
    "    plt.tight_layout(); plt.savefig(out_png)\n",
    "    print(f\"Saved summary bar chart to {out_png}\")\n",
    "\n",
    "def plot_latency_series(metrics_files, labels, out_png):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    for f, lab in zip(metrics_files, labels):\n",
    "        with open(f) as jf:\n",
    "            m = json.load(jf)\n",
    "            if 'latencies' in m:\n",
    "                ys = m['latencies']\n",
    "            else:\n",
    "                # If not present, skip\n",
    "                continue\n",
    "            xs = list(range(len(ys)))\n",
    "            plt.plot(xs, ys, label=lab, alpha=0.8)\n",
    "    plt.xlabel('Frame'); plt.ylabel('Latency (ms)'); plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(out_png)\n",
    "    print(f\"Saved latency series plot to {out_png}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    metrics_files = [\n",
    "        'baseline_coco_metrics.json',\n",
    "        'trt_coco_metrics.json',\n",
    "        'trt_fp16_coco_metrics.json'\n",
    "    ]\n",
    "    labels = ['PyTorch', 'TensorRT', 'TensorRT FP16']\n",
    "    plot_latency_fps_bar(metrics_files, labels, 'latency_fps_bar.png')\n",
    "    plot_latency_series(metrics_files, labels, 'latency_series.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ea70ca",
   "metadata": {},
   "source": [
    "### visualize_task_performance.py (PR/Recall/F1 Curves) [<a href='#ref-9'>9</a>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ff3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAB_DIR/visualize_task_performance.py\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_pr_f1_multi(curves_json_list, labels, out_png):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    for curves_json, label in zip(curves_json_list, labels):\n",
    "        with open(curves_json) as f:\n",
    "            d = json.load(f)\n",
    "        conf = d['precision_curve'][0]\n",
    "        p = d['precision_curve'][1]\n",
    "        r = d['recall_curve'][1]\n",
    "        f1 = d['f1_curve'][1]\n",
    "        plt.plot(conf, p, label=f'{label} Precision')\n",
    "        plt.plot(conf, r, label=f'{label} Recall')\n",
    "        plt.plot(conf, f1, label=f'{label} F1')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png)\n",
    "    print(f'Saved {out_png}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    curves_json_list = [\n",
    "        'baseline_pr_curve.json',\n",
    "        'trt_pr_curve.json',\n",
    "        'trt_fp16_pr_curve.json'\n",
    "    ]\n",
    "    labels = ['PyTorch', 'TensorRT', 'TensorRT FP16']\n",
    "    plot_pr_f1_multi(curves_json_list, labels, 'pr_f1_curves.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b253412d",
   "metadata": {},
   "source": [
    "## Task 5: Performance Profiling with Nsight and TensorRT [<a href='#ref-7'>7</a>, <a href='#ref-8'>8</a>, <a href='#ref-5'>5</a>, <a href='#ref-10'>10</a>]\n",
    "\n",
    "Instructions: [<a href='#ref-7'>7</a>]\n",
    "\n",
    "- Add NvtxRange around inference loops; use Nsight Systems with capture-range=nvtx for focused timelines [<a href='#ref-7'>7</a>].\n",
    "- Use Nsight Compute on short runs to extract kernel-level metrics and inspect hotspots [<a href='#ref-8'>8</a>].\n",
    "- Use trtexec with detailed verbosity and JSON exports for per-layer latencies from the ONNX engine [<a href='#ref-5'>5</a>, <a href='#ref-10'>10</a>].\n",
    "- Install Nsight Systems GUI on a desktop (lab computer) or laptop (your own computer) following the guideline at https://developer.nvidia.com/nsight-systems/get-started.\n",
    "- View and Visualize major metrics and analyze the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714ef072",
   "metadata": {},
   "source": [
    "### Add NVTX annotations to inference loops\n",
    "Add the following class and relevant annotations to the inference loop of the following scripts:\n",
    "- `baseline_infer_pytorch.py`\n",
    "- `task1_infer_trt.py`\n",
    "- `task3_infer_trt_fp16.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f689367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAB_DIR/profile_wrappers.py\n",
    "import torch\n",
    "import torch.cuda.nvtx as nvtx\n",
    "\n",
    "class NvtxRange:\n",
    "    def __init__(self, msg): self.msg = msg\n",
    "    def __enter__(self): nvtx.range_push(self.msg)\n",
    "    def __exit__(self, exc_type, exc, tb): nvtx.range_pop()\n",
    "\n",
    "# Example usage inside an inference loop:\n",
    "# with NvtxRange('inference_step'):\n",
    "#     torch.cuda.synchronize()\n",
    "#     # run inference here\n",
    "# torch.cuda.synchronize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfbb5d2",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Profiling Analysis: Step-by-Step Guide\n",
    "\n",
    "#### **A. Nsight Systems (CPU–GPU Timeline, NVTX Ranges, Synchronization)**\n",
    "\n",
    "1. **Annotate Your Code:**\n",
    "   - Add `NvtxRange` context managers around your inference loop as shown in `profile_wrappers.py`.\n",
    "   - Example:\n",
    "     ```python\n",
    "     from profile_wrappers import NvtxRange\n",
    "     with NvtxRange('inference_step'):\n",
    "         torch.cuda.synchronize()\n",
    "         # run inference here\n",
    "         torch.cuda.synchronize()\n",
    "     ```\n",
    "\n",
    "2. **Run Profiling:**\n",
    "   - In your terminal, execute:\n",
    "     ```bash\n",
    "     nsys profile -o nsys_report --trace=cuda,nvtx,osrt -c nvtx --capture-range=nvtx --capture-range-end=stop python <your_script.py>\n",
    "     ```\n",
    "   - Replace `<your_script.py>` with the script you want to profile (e.g., `baseline_infer_pytorch.py`).\n",
    "\n",
    "3. **Open the Report:**\n",
    "   - Transfer the `.nsys-rep` file to your desktop/laptop if needed.\n",
    "   - Open with Nsight Systems GUI (`nsys-ui nsys_report.nsys-rep`).\n",
    "\n",
    "4. **Analyze:**\n",
    "   - Use the **Summary** and **Timeline** views.\n",
    "   - Look for:\n",
    "     - NVTX ranges marking inference steps.\n",
    "     - Overlap between CPU and GPU activity.\n",
    "     - CUDA kernel launches and memory transfers.\n",
    "     - Host synchronizations (`cudaDeviceSynchronize`).\n",
    "   - Identify bottlenecks such as excessive synchronization or poor overlap.\n",
    "\n",
    "---\n",
    "\n",
    "#### **B. Nsight Compute (Kernel-Level Analysis, Occupancy, Memory)**\n",
    "\n",
    "1. **Short Profiling Run:**\n",
    "   - Run a short inference (few frames or images).\n",
    "   - Profile with:\n",
    "     ```bash\n",
    "     ncu --set full --target-processes all -o ncu_report python <your_script.py>\n",
    "     ```\n",
    "\n",
    "2. **Open the Report:**\n",
    "   - Use Nsight Compute GUI (`ncu-ui ncu_report.ncu-rep`).\n",
    "\n",
    "3. **Analyze:**\n",
    "   - Review **Speed Of Light**, **Occupancy**, and **Memory Workload** sections.\n",
    "   - Check for:\n",
    "     - Kernel execution times.\n",
    "     - SM occupancy and eligible warps.\n",
    "     - Memory throughput and stall reasons.\n",
    "   - Identify if your workload is memory-bound or compute-bound.\n",
    "\n",
    "---\n",
    "\n",
    "#### **C. TensorRT trtexec (Layer Latency, Tactics, Precision)**\n",
    "\n",
    "1. **Export and Profile Engine:**\n",
    "   - Use trtexec to profile your TensorRT engine:\n",
    "     ```bash\n",
    "     trtexec --onnx=Lab5/yolov8x.onnx --shapes=input:1x3x640x640 \\\n",
    "       --profilingVerbosity=detailed --dumpLayerInfo --dumpProfile \\\n",
    "       --separateProfileRun --exportTimes=times.json --exportProfile=profile.json\n",
    "     ```\n",
    "\n",
    "2. **Analyze Output:**\n",
    "   - Open `times.json` and `profile.json` in a text editor or import into Python for visualization.\n",
    "   - Look for:\n",
    "     - Per-layer latency breakdown.\n",
    "     - Tactics and precision used (FP32/FP16/INT8).\n",
    "     - Identify layers with highest latency.\n",
    "\n",
    "3. **Compare FP16/INT8:**\n",
    "   - Repeat profiling with FP16 or INT8 enabled.\n",
    "   - Compare speedups and check for any accuracy drop using your validation metrics.\n",
    "\n",
    "---\n",
    "\n",
    "**Tip:** Always annotate and save screenshots and plots, and highlight key findings and recommendations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af5f15c",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- <a id='ref-1'></a>[1] Ultralytics YOLO — Model Export & Predict Usage — https://docs.ultralytics.com/modes/export/\n",
    "- <a id='ref-2'></a>[2] Ultralytics Exporter API reference — https://docs.ultralytics.com/reference/engine/exporter/\n",
    "- <a id='ref-3'></a>[3] Ultralytics YOLOv8 model usage — https://docs.ultralytics.com/models/yolov8/\n",
    "- <a id='ref-4'></a>[4] NVIDIA TensorRT Best Practices — https://docs.nvidia.com/deeplearning/tensorrt/latest/performance/best-practices.html\n",
    "- <a id='ref-5'></a>[5] TensorRT trtexec wrapper & flags — https://github.com/NVIDIA/TensorRT/tree/main/samples/trtexec\n",
    "- <a id='ref-6'></a>[6] NVIDIA torch2trt GitHub — https://github.com/NVIDIA-AI-IOT/torch2trt\n",
    "- <a id='ref-7'></a>[7] Nsight Systems User Guide — https://docs.nvidia.com/nsight-systems/UserGuide/index.html\n",
    "- <a id='ref-8'></a>[8] Nsight Compute CLI/UI — https://docs.nvidia.com/nsight-compute/NsightComputeCli/index.html\n",
    "- <a id='ref-9'></a>[9] Ultralytics — Model Validation & Metrics — https://docs.ultralytics.com/modes/val/\n",
    "- <a id='ref-10'></a>[10] trtexec timing/throughput clarifications — https://forums.developer.nvidia.com/t/need-some-precisions-about-trtexec-measures/173133"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
